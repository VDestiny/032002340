# 一、PSP表格
## 1. 在开始实现程序之前，在附录提供的PSP表格记录下你估计将在程序的各个模块的开发上耗费的时间。
## 2. 在你实现完程序之后，在附录提供的PSP表格记录下你在程序的各个模块上实际花费的时间。
---
| PSP2.1 | Personal Software Process Stages | 预估耗时（小时） | 实际耗时（小时） |
| ---- | ---- | ---- | ---- |
| Planning | 计划 | 1 | 0.8 |
| Estimate | 估计这个任务需要多少时间 | 72 | 96 |
| Development | 开发 | 50 | 72 |
| Analysis | 需求分析 (包括学习新技术) |  |  |
| Design Spec | 生成设计文档 |  |  |
| Design Review | 设计复审 |  |  |
| Coding Standard | 代码规范 (为目前的开发制定合适的规范） |  |  |
| Design | 具体设计 |  |  |
| Coding | 具体编码 |  |  |
| Code Review | 代码复审 |  |  |
| Test | 测试（自我测试，修改代码，提交修改） |  |  |
| Reporting | 报告 |  |  |
| Test Repor | 测试报告 |  |  |
| Size Measurement | 计算工作量 |  |  |
|  Postmortem & ProcessImprovement Plan | 事后总结, 并提出过程改进计划 |  |  |
|  | 合计 |  |  |
---

# 二、任务要求的实现
## 1. 项目设计与技术栈。从阅读完题目到完成作业，这一次的任务被你拆分成了几个环节？你分别通过什么渠道、使用什么方式方法完成了各个环节？列出你完成本次任务所使用的技术栈。
从阅读完题目到完成作业，一共分为了四大部分。    
第一部分是学习：通过b站和慕课网学习python，学习爬虫，学习利用python制作excel和可视化，~~在知识的海洋中遨游~~。   
第二部分是编写代码：通过之前的学习和在CSDN中查找的资料，不断修改代码，最终成功爬取卫健委的数据。  
第三部分是制作可视化：通过学习和CSDN中的资料制作数据可视化。  
第四部分是代码测试：在完成代码后，我通过不断运行测试代码完成度和契合度，再通过细小的修改进一步优化代码。
## 2. 爬虫与数据处理。说明业务逻辑，简述代码的设计过程（例如可介绍有几个类，几个函数，他们之间的关系），并对关键的函数或算法进行说明。
---
主函数：  
{   
    
    if __name__ == '__main__':
    #创建excel
    make_excel()    #仅在第一次运行程序时启用

    #从42页数据中获取二级url的尾部数据
    for i in range(1, 5):   #规定网页的页码范围
        #计算一级页面url
        url = calculate_one(i)     
        
        #获取二级页面url
        url = calculate_two(url)
}  
计算一级url：  
{

    def calculate_one (page):
    #根据页码计算出一级url
    if page==1:     
        url = 'http://www.nhc.gov.cn/xcs/yqtb/list_gzbd.shtml'
    else:
        url = urll.format('_'+str(page))
    #利用一级url进入界面获取二级url
    return url
}  
计算二级url并在单个循环中进行数据爬取：
{

    def calculate_two (url):          
    #获取二级页面url
    response = requests.get(url=url, headers=headers)
    #将一级界面数据内容存入page_text
    page_text = response.text
    #在一级中依次进入24个二级页面
    for i in range(0, 24):  #一页中存在24个二级页面
        soup = BeautifulSoup(page_text, 'lxml')
        s = soup.select('.list> ul a')[i]['href']
        url = urlll + s
        print(url)
        #算出url后爬取数据
        acquire_data(url)
}  
数据爬取：
{  

    def acquire_data(url):      
    #爬取数据
    response = requests.get(url=url, headers=headers)
    page_text = response.text
    #将二级界面数据内容进行处理
    soup = BeautifulSoup(page_text, 'lxml')
    s = soup.select('.con > p')
    a = ''
    for i in s:
        b = ''.join(i.text)
        a = a + b       #a中存有页面数据
    #取出日期
    date = str(get_date(page_text))
    #取出当日本土确诊
    sure = str(get_day_all_data_sure(a))
    #取出当日本土无症状
    unsure = str(get_day_all_data_unsure(a))
    #将每个省份的情况分别储存
    get_day_provincedata(a)
    #将港澳台的情况分别存储
    get_day_HKAMTW(a)
    #写入excel
    write_excel(date, sure, unsure)
}  
获取日期：
{

    def get_date(page_text):
    #获取日期
    soup = BeautifulSoup(page_text, 'lxml')
    s = soup.select('title')
    #利用正则表达式提取
    pattern = re.compile(r'\d+月\d+日')
    z = pattern.findall(str(s))
    date = "".join(z)
    print(date)
    return (date)
}  
获取本土当日新增确诊：
{

    def get_day_all_data_sure(a):
    #获取本土当日疫情情况         
        #获取本土新增确诊
    pattern = re.compile('新疆生产建设兵团报告新增确诊病例\d+例')
    result = pattern.findall(str(a))
    pattern = re.compile('\d+')
    z = pattern.findall(str(result))
    sure = "".join(z)
    print(sure)
    return (sure)
}  
获取本土当日新增无症状：
{

    def get_day_all_data_unsure(a):
    #获取本土当日疫情情况  
        #获取本土无症状新增确诊
    pattern = re.compile('新疆生产建设兵团报告新增无症状感染者\d+例')
    result = pattern.findall(a)
    pattern = re.compile('\d+')
    z = pattern.findall(str(result))
    unsure = "".join(z)
    print(unsure)
    return (unsure)
}  
存储各省份疫情情况：
{

    def get_day_provincedata(a):
    #获取各省份当日疫情情况
        #获取各省份新增确诊
    pattern = re.compile('本土病例\d+例（.*?），含')
    result = pattern.findall(a)
    for i in range(0,34):
        pattern = re.compile (r'{}\d+'.format(Province[i]))
        result2 = pattern.findall(str(result))
        pattern = re.compile("\d+")
        z = pattern.findall(str(result2)) 
        L[i] = "".join(z)
        
        print(Province[i], L[i])
}  
存储港澳台疫情情况：
{

    def get_day_HKAMTW(a):
    #获取香港当日疫情情况
    pattern = re.compile('香港特别行政区\d+例')
    result = pattern.findall(a)
    pattern = re.compile("\d+")
    z = pattern.findall(str(result)) 
    L[26] = "".join(z)

    #获取澳门当日疫情情况
    pattern = re.compile('澳门特别行政区\d+例')
    result = pattern.findall(a)
    pattern = re.compile("\d+")
    z = pattern.findall(str(result)) 
    L[33] = "".join(z)

    #获取台湾当日疫情情况
    pattern = re.compile('台湾地区\d+例')
    result = pattern.findall(a)
    pattern = re.compile("\d+")
    z = pattern.findall(str(result)) 
    L[32] = "".join(z)
}  
将数据写入excel表格：
{

    def write_excel(date, sure, unsure):
    xfile = vv.load_workbook('COVID2.xlsx')
    ws = xfile.get_sheet_by_name('Sheet')
    ws.append([str(date), str(sure), str(unsure)]+ L)
    xfile.save('COVID2.xlsx')
}  
创建excel表格：
{

    def make_excel():
    xfile = vv.Workbook() # 创建工作簿对象
    ws = xfile['Sheet'] # 创建子表
    ws.append(['日期/省份', '本土新增', '本土无症状']+ Province)
    xfile.save('COVID2.xlsx')
}  

## 3. 数据统计接口部分的性能改进。记录在数据统计接口的性能上所花费的时间，描述你改进的思路，并展示一张性能分析图（例如可通过VS 2019/JProfiler的性能分析工具自动生成），并展示你程序中消耗最大的函数。
之前的代码是将二级url从网上爬取下来后保存，然后再在保存的html文件中进入二级页面爬取数据，经过性能改进后，我利用循环在每一页一级页面中依次提取二级url，在二级页面提取完所需数据后进入才进入二级页面，中间不将数据进行保存。
## 4. 每日热点的实现思路。简要介绍实现该功能的算法原理，可给出必要的步骤流程图、数学公式推导和核心代码实现，并简要谈谈所采用算法的优缺点与可能的改进方案。
利用代码突出每日新增较多的省份，突出较昨日继续增长较多或大幅减少的省份。每日新增超过30列为每日新增较多的省份，较昨日新增增长超过10人列为继续增长较多，较昨日新增减少超过30人列为大幅减少的省份。

## 5. 数据可视化界面的展示。在博客中介绍数据可视化界面的组件和设计的思路。
可视化代码：  
{

    data = pandas.read_excel('COVID3.xlsx')
    date = input('请输入您想要查询的日期：')
    day_data = list(data[date])
    province = list(data["日期/省份"])
    list = [list(z) for z in zip(province, day_data)]  
    print(list)
    s = "2022年{}本土疫情情况"
    s = s.format(date)
    c = (
        Map(init_opts=options.InitOpts(width="1000px", height="600px"))
        .set_global_opts(
        title_opts=options.TitleOpts(title=s),
        visualmap_opts=options.VisualMapOpts(
            min_=1,
            max_=50,
            range_text = ['感染人数:', ''],  #分区间
            is_piecewise=True,  #定义图例为分段型
            pos_top= "middle",  #分段位置
            pos_left="left",
            orient="vertical",
            split_number=10  #分成10个区间
        )
    )
    .add("新增人数",list,maptype="china")
    .render("Map2.html")
}

---
# 三、心得体会
在本次单人作业中，我通过网站初步学习了python和爬虫的基本知识，之前我对python一窍不通，这次的项目让我又接触了一门语言，同时也让我发现了语言的学习是互通的，拥有c语言的基础让我对python更容易上手，更发现了python的强大。  
在爬虫的学习过程中，我体会到了痛并快乐着，每一次的代码错误都让我头疼，但最终数据被我从网络上爬取下来后又是感到~~无比的快乐~~，但当我以为从网站上爬取数据是最终的胜利时，数据处理又给我了一个响亮的耳光。  
在学习的过程中，我终于见到了正则表达式，也意识到了正则表达式的强大之处，以前总是听到同学说到正则很厉害，我也一直以为这是我遥不可及的东西，但通过本次作业，我能够初步的掌握正则表达式，以至于之后我每次对文本的处理都会第一时间想到正则表达式（真的太好用了！！！）。  
这次的单人项目，让我在两周的生活里活得多姿多彩（~~痛不欲生~~），更是在中秋佳节与代码一起分享月饼，这两周所取得的收获是巨大的，我接触了python、爬虫、excel处理和正则表达式，这些都是以前的我很难接触到的工具，通过本次项目，让我更有兴趣进一步学习python等工具，这些将在未来为我提供巨大的帮助。